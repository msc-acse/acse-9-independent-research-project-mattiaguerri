

To Do:

- Check results obtained training UNet07 with smaller dataset.

- Check the results obtained with UNet08






#####  Experiments Log  ###############################################
18/07
Best model for now is UNet07. I found two issues in it. 
1) The last downblock, the sizes are 4,6, they should be 3,5. Testing now what I get if this is fixed.
2) Activation after the transposed convolution. Test whether it is better to do it or not.

20/07
Tested UNet08. [64, 128, 256, 512, 1024, 2048, 4096]. BS = 8.
After 58 epochs, val loss = 14.25 (UNet07 val loss = 12.77).

21/07
UNetX 

UNetX_1 Instead of doing pooling i do two convolutions.
#######################################################################







##### Directories Log #################################################
- MachineWorkFlow_00 ------------ Original Code.

- MachineWorkFlow_01 ------------ Copy of MachineWorkFlow_00.
                                  Added Normalization + Standardization + Batch Normalization.

- MachineWorkFlow_02 ------------ Copy of MachineWorkFlow_00. Added various architectures. They are in the file networks.py.
                                  Changed how the architecture is defined. It is here defined in the parameters.ini.
                                  Added Normalization + Standardization + Batch Normalization.
                                  Adding the U-Net.

- MachineWorkFlowDev00 ---------- Copy of MachineWorkFlow_00
                                  While trying to add U-Net I messed up MachineWorkFlow_02 irrimediabely. 
                                  I re-start from scratch.

- MachineDev00 ------------------ Copy of /programming/aclowes/for_production/seismic2seismic (08-07-2019).

- MachineDev02 ------------------ Test weight decay. Training finished.

- MachineRunBaseLineFastTrack --- Baseline model, trained on fast track data.

- MachineRunBaseLine ------------ Baseline model, trained on full preduction data.

- MachineRunUnet04 -------------- [64, 128, 256, 512, 1024, 2048].
                                  In this folder also UNet08.
                                  [64, 128, 256, 512, 1024, 2048, 4096]. BS = 8. Trained till epoch 58.

- MachineRunUnet07 -------------- [64, 96, 144, 216, 324, 486, 729, 1094, 1641]. 2X 1080. BS 4.
                                  Metrics:
                                      UNet0700: run with 4X 1080 ti. BS = 8.
                                      UNet0701: run with 2X 1080 ti. BS = 4.
                                      UNet0702: run with 2X 1080 ti. BS = 4. Trained with train_02.csv.
                                      UNet0703: run with 4X 1080 ti. BS = 8. Trained with train_02.csv.

- MachineRunUNet07Epo300 -------- Re-starting the training from model 167 of the folder MachineRunUNet07.
                                  Metrics are in folder 1.
                                  Minimum reached for model 59.
                                  Training stopped after epoch 141 because i was overfitting.

- MachineRunUNet07Epo300Tra ----- Re-starting training from model 59 of MachineRunUNet07Epo300.
                                  The training uses the L1 loss function.
                                  Regularization is set to 1e-3.
                                  Metrics are in folder 2.
                                  The metrics show no improvement.
                                  I restart the training. No regularization. Learing rate 1e-5.
                                  Metrics are in folder 3. Stopped epoch 32.

- MachineRunUnet07FasTraDeg ----- [64, 96, 144, 216, 324, 486, 729, 1094, 1641]. 2X 1080. BS 4.

- MachineRunUNet07FasTraDegTra -- Trained completed.
                                  Re-training with transfer learning using another loss function.
                                  Trained on fast track deghosted dataset.

- MachineRunUNet07Deg ----------- Trained on deghosted dataset.

- MachineRunUNet07DegTra -------- Re-starting training with L1 loss. Stopped at epoch 119.

- MachineRunUNet07DegTra01 ------ Re-starting training with L1 loss. lr=1e-5. Stopped epoch 113.


- MachineRunUNet07Verify -------- Test the correction in the last DownBlock. Effect neglectable.

- MachineRunUNet07BigBat -------- Batch Size = 12. After 200 epochs val still going down.
                                  Restart training using model 200 from first run.

- Machine1507Dev00 -------------- Copy of the pipeline from /tpanew/shou/seismic2seismic
                                  The email about this pipeline was received 15/07/2019.
                                  Working to add UNet with downsampling on only one direction.

- Machine1507RunUNet07Hor ------- Pooling only horizontally.
                                  Trained with BS=4. UNet07 performs better.

#######################################################################






#######################################################################
Questions:
- In the UNet, do i need to activate the result of the transposed convolution?
- In the UNet with batch normalization, should i batch norm the output of the transpose convolution?
#######################################################################





#######################################################################
Useful Links
- Review Semantic Segmentation Algorithms: https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57
- Batch Normalization: https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/
- FCN paper: https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf
- Blog article about VGG16: https://neurohive.io/en/popular-networks/vgg16/
- VGG16 Article: https://arxiv.org/abs/1409.1556
- Residual Network Paper: https://arxiv.org/pdf/1512.03385.pdf
- Autonomous cars economic impact: https://www.theverge.com/2017/6/1/15725516/intel-7-trillion-dollar-self-driving-autonomous-cars
- U-Net implementation: https://www.kaggle.com/witwitchayakarn/u-net-with-pytorch/notebook
- Custom Kernel https://discuss.pytorch.org/t/custom-convolutional-kernel/30716
#######################################################################





##### TESTS ###########################################################
This is all using the first csv files for training, validation and inference.
Epochs (E) = 200 unless specified.
Batch Size (BS) = 4 unless specified.

- output_00, CGG8, .............[128, 64, 64, 64, 64, 32, 16], ST 9:20, ET 00:11 (+1),  TL 1.7864690691976050, VL 3.3999928074914054
- output_01, CGG8NSBN,     ST 19:00, ET 10:15(+1), .................................... TL 1.5759501973517627, VL 3.1531237524908944
- output_02, CGG12, ................................................................... TL 1.6948972091653180, VL 3.2641788173366235
- output_03, CGG12BN,      ST 10:50, ET 19.15(+1), .................................... TL 1.5045548051663267, VL 3.1070716767697720
- output_04, CGG15, 100 E, ST 11:00, ET 01:02(+1), .................................... TL 1.7760124316020889, VL 3.3942056024396740
- output_05, CGG15NSBN,    ST 10:30, ET 23:30, ........................................ TL 1.1322007296847650, VL 3.2677156538576693

- output_UNet_01, .............. [64, 128, 256, 512, 1024]...............TL 0.8330796942824409, VL 2.8687951886976086
- output_UNet_02, .............. [64, 128, 256, 512, 1024, 1024, 1024]...TL 0.5091864192972377, VL 2.8062365570583860
- output_UNet_04, .............. [64, 128, 256, 512, 1024, 2048] ....... TL 0.6378958375275541, VL 2.9634264127628223
- output_UNet_05, BS=2 ......... [64, 128, 256, 512, 1024, 2048, 4096]...TL 0.6041997219699851, VL 2.6939100300943530
- output_UNet_06, .............. [64, 128, 256, 512, 1024], carry,.......TL 0.8556496554356313, VL 2.9340661731926170
- output_UNetRes_01,............ [64, 128, 256, 512, 1024],..............TL 0.5897464164936083, VL 2.9630362955299585
#######################################################################


#######################################################################
source /cgv/geovation/2-db9/rel/python/venv/3.6/ml.003/bin/activate.csh
setenv PATH /cgv/geovation/2-db9/rel/bin:$PATH
setenv LD_LIBRARY_PATH /programming/imikhale/local/cuda/cuda-9.0/lib64/:/programming/imikhale/local/cuda/cudnn-9.0-v7.4/lib64/

cd /flat1/3dwoswaz/guerri/

#######################################################################
#######################################################################
source /programming/imikhale/local/bin/venv_db9/bin/activate.csh
setenv LD_LIBRARY_PATH /programming/imikhale/local/cuda/cuda-9.0/lib64/:/programming/imikhale/local/cuda/cudnn-9.0-v7.4/lib64/
tensorboard --logdir='./output/' --port=6006

#######################################################################





##### Running Tests ###################################################

Machine1507RunUNet07Skip, training on redg37022 stopped at 329 bacuse overfitting
Machine1507RunUNetX. Training on redg99203. ACTIVE
Machine1507RunUNet07BN. Training on redg36024. ACTIVE
Machine1507RunUNet07Skip01. BS = 8. Training on redg37023. ACTIVE
Machine1507RunUNet_X1. BS = 8. Training on redg37022. ACTIVE

########################################################################























